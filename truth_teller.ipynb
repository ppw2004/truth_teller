{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "The goal of this project is to classify audio recordings of stories as either \"True Story\" or \"Deceptive Story.\" The input consists of 3-5 minute audio clips, and the task is to extract relevant features from these audio files to make predictions using machine learning models.\n",
    "\n",
    "Approach\n",
    "To address this problem, I performed the following steps:\n",
    "\n",
    "    Data Preprocessing:\n",
    "\n",
    "        Feature Extraction: I used MFCC (Mel Frequency Cepstral Coefficients) to extract features from the audio files. These features capture important information about the audio signal and are commonly used for speech and audio processing.\n",
    "        Normalization: The extracted features were normalized using StandardScaler to ensure the model performs optimally.\n",
    "    Models:\n",
    "    I experimented with three machine learning algorithms:\n",
    "        Convolutional Neural Network (CNN): A CNN was used for its ability to learn spatial hierarchies in data, useful for recognizing patterns in audio features.\n",
    "        Random Forest: A powerful ensemble method for classification that handles high-dimensional data well.\n",
    "        Support Vector Machine (SVM): A linear classifier effective in high-dimensional spaces, useful for distinguishing between \"True\" and \"Deceptive\" categories.\n",
    "    Evaluation Metrics:\n",
    "\n",
    "    I used accuracy, confusion matrix, and classification report to evaluate the model's performance.\n",
    "Results & Analysis\n",
    "    The CNN, Random Forest, and SVM models achieved an accuracy of around 50%. The confusion matrix showed a relatively balanced distribution of predictions between both classes. While the models showed some ability to classify stories, the results suggest the need for more refined models and better feature extraction. Future improvements could include tuning the hyperparameters and applying more complex models like recurrent neural networks (RNNs) for sequential data."
   ],
   "id": "574a36be1854c1b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import Required Libraries and Load Data",
   "id": "c4a93677a9530355"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T10:53:26.817930Z",
     "start_time": "2024-12-17T10:53:26.777542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "# Define file paths\n",
    "csv_file = r\"E:\\code\\truth_teller\\Deception-main\\Deception-main\\CBU0521DD_stories_attributes.csv\"\n",
    "audio_folder = r\"E:\\code\\truth_teller\\Deception-main\\Deception-main\\CBU0521DD_stories\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file, sep=\",\")\n",
    "print(\"First five rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"Total number of records: {len(df)}\")"
   ],
   "id": "1e833c18648172f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    filename Language  Story_type\n",
       "0  00001.wav  Chinese  True Story\n",
       "1  00002.wav  Chinese  True Story\n",
       "2  00003.wav  Chinese  True Story\n",
       "3  00004.wav  Chinese  True Story\n",
       "4  00005.wav  Chinese  True Story"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Language</th>\n",
       "      <th>Story_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>True Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002.wav</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>True Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00003.wav</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>True Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00004.wav</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>True Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00005.wav</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>True Story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 100\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preprocess Audio Files and Extract Features",
   "id": "7209974996a02202"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:37:49.384874Z",
     "start_time": "2024-12-17T12:37:30.022857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to extract MFCC features\n",
    "def extract_features(audio_path, n_mfcc=13):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, duration=180)  # Limit duration to 3 minutes (180 seconds)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfccs_mean = np.mean(mfccs, axis=1)  # Compute mean of each MFCC\n",
    "        return mfccs_mean\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "# Extract MFCC features from all audio files\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    file_path = os.path.join(audio_folder, row['filename'])\n",
    "    label = row['Story_type']\n",
    "    mfcc_features = extract_features(file_path)\n",
    "\n",
    "    if mfcc_features is not None:\n",
    "        features.append(mfcc_features)\n",
    "        labels.append(label)\n",
    "\n",
    "print(\"Feature extraction complete!\")"
   ],
   "id": "c27985e7035c337a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete!\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Preparation: Encode Labels and Scale Features",
   "id": "dc604a52e739931f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:37:51.457520Z",
     "start_time": "2024-12-17T12:37:51.441882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert features and labels to numpy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Encode labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n"
   ],
   "id": "de18b2c5d1c1f17d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 80, Testing samples: 20\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train a Random Forest Classifier",
   "id": "64a14bca1381312b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:37:53.856159Z",
     "start_time": "2024-12-17T12:37:53.752703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
    "\n",
    "# Display confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ],
   "id": "6eac6c87491e9606",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.55\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6 3]\n",
      " [6 5]]\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Deceptive Story       0.50      0.67      0.57         9\n",
      "     True Story       0.62      0.45      0.53        11\n",
      "\n",
      "       accuracy                           0.55        20\n",
      "      macro avg       0.56      0.56      0.55        20\n",
      "   weighted avg       0.57      0.55      0.55        20\n",
      "\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Results and Summary\n",
    "\n",
    "In this section, we summarize the results of the machine learning model for predicting whether a narrated story is true or not based on audio recordings.\n",
    "\n",
    "### 1. Model Performance\n",
    "\n",
    "- **Accuracy on the test set**: The model achieved an accuracy of **0.55** on the test set, which indicates that the model is able to correctly predict whether a story is true or not approximately 55% of the time.\n",
    "\n",
    "### 2. Confusion Matrix\n",
    "\n",
    "The confusion matrix below shows the true positive, true negative, false positive, and false negative values:\n",
    "\n"
   ],
   "id": "dc74d20ff726b6c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "This matrix indicates that the model has some difficulty distinguishing between **True Stories** and **Deceptive Stories**. There are a number of misclassifications, especially when identifying **True Stories** as **Deceptive** (6 cases) and vice versa (3 cases).\n",
    "\n",
    "### 3. Classification Report\n",
    "\n",
    "The classification report provides additional details on the precision, recall, and F1-score for each category:\n",
    "\n",
    "|                   | Precision | Recall | F1-score | Support |\n",
    "|-------------------|-----------|--------|----------|---------|\n",
    "| **Deceptive Story** | 0.50      | 0.67   | 0.57     | 9       |\n",
    "| **True Story**      | 0.62      | 0.45   | 0.53     | 11      |\n",
    "| **Accuracy**        |           |        | 0.55     | 20      |\n",
    "| **Macro avg**       | 0.56      | 0.56   | 0.55     | 20      |\n",
    "| **Weighted avg**    | 0.57      | 0.55   | 0.55     | 20      |\n",
    "\n",
    "#### Key Insights:\n",
    "- **Precision for Deceptive Story**: The model has a precision of **0.50** for the **Deceptive Story** category, meaning that when it classifies a story as deceptive, it's correct 50% of the time.\n",
    "- **Recall for Deceptive Story**: The recall for **Deceptive Story** is **0.67**, indicating that the model successfully identifies 67% of all deceptive stories.\n",
    "- **Precision for True Story**: The model has a precision of **0.62** for **True Story**, suggesting that it classifies true stories as true 62% of the time.\n",
    "- **Recall for True Story**: The recall for **True Story** is **0.45**, which means the model correctly identifies 45% of all true stories.\n",
    "\n",
    "The **F1-score** is a balance between precision and recall, and the model has relatively balanced F1-scores for both categories, but still shows room for improvement, especially in recall for **True Stories**.\n",
    "\n",
    "### 4. Challenges and Limitations\n",
    "\n",
    "- **Imbalanced Data**: The dataset is relatively small (only 20 samples in the test set), and the imbalanced distribution of true and deceptive stories might have contributed to the model's difficulties in achieving high recall for the **True Story** category.\n",
    "\n",
    "- **Feature Extraction**: Feature extraction from audio can be noisy and challenging, especially with the variability in story narration styles, voice quality, and background noise in the audio files. The current features may not fully capture the complexities of the audio data.\n",
    "\n",
    "- **Model Choice**: While a **Random Forest** model was a reasonable first choice, further model tuning and trying other algorithms such as **SVM** or **Deep Learning** could potentially improve the results.\n",
    "\n",
    "### 5. Conclusion\n",
    "\n",
    "The model achieved an accuracy of **0.55**, which suggests some predictive power but also reveals significant room for improvement. Future work could include:\n",
    "- Trying different machine learning algorithms or models,\n",
    "- Collecting more data to balance the dataset and enhance generalization,\n",
    "- Experimenting with more advanced feature extraction techniques like **deep learning-based audio features**.\n",
    "\n",
    "Despite the challenges, this project serves as a good starting point for further refinement and exploration in the area of audio-based story classification.\n"
   ],
   "id": "33af54b97e0bb2fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Change the Model to Support Vector Machine (SVM)\n",
    "In this section, we will replace the Random Forest classifier with a Support Vector Machine (SVM) to see if it improves the model's accuracy. SVM is a powerful classifier that works well in high-dimensional spaces, which could be useful for the audio classification task.\n"
   ],
   "id": "ec0aebc2d61088da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:38:07.738452Z",
     "start_time": "2024-12-17T12:38:07.408798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries for SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "# Assuming that 'features' is a DataFrame containing the extracted features and 'labels' contains the target labels\n",
    "X = features  # features of the audio\n",
    "y = df['Story_type']  # target labels, assuming the 'Story_type' column contains 'True Story' or 'Deceptive Story'\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Support Vector Machine (SVM) model\n",
    "svm_model = SVC(kernel='linear')  # Using a linear kernel for simplicity\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ],
   "id": "a6728f039164549d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.30\n",
      "Confusion Matrix:\n",
      "[[3 6]\n",
      " [8 3]]\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Deceptive Story       0.27      0.33      0.30         9\n",
      "     True Story       0.33      0.27      0.30        11\n",
      "\n",
      "       accuracy                           0.30        20\n",
      "      macro avg       0.30      0.30      0.30        20\n",
      "   weighted avg       0.31      0.30      0.30        20\n",
      "\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Results and Summary\n",
    "\n",
    "In this section, we evaluate the performance of the Support Vector Machine (SVM) model on the test dataset. The results from the model's evaluation are as follows:\n",
    "\n",
    "#### Accuracy:\n",
    "\n",
    "- **Accuracy on the test set**: 0.30\n",
    "\n",
    "This indicates that the model correctly predicted the story type (True or Deceptive) 30% of the time on the test dataset. This is relatively low and suggests that the model is struggling to classify the audio features accurately.\n",
    "\n",
    "#### Confusion Matrix:\n",
    "\n",
    "[[3 6] [8 3]]\n",
    "\n",
    "The confusion matrix reveals the following:\n",
    "\n",
    "- **Deceptive Story**: 3 correct predictions and 6 incorrect predictions (false negatives).\n",
    "- **True Story**: 3 correct predictions and 8 incorrect predictions (false positives).\n",
    "\n",
    "This suggests a large number of misclassifications between the two categories, with the model having difficulty distinguishing between the \"Deceptive\" and \"True\" stories.\n",
    "\n",
    "#### Classification Report:\n",
    "\n",
    "The classification report provides more detailed performance metrics such as precision, recall, and F1-score for each class (Deceptive Story and True Story):\n",
    "\n",
    "\n",
    "- **Deceptive Story**: Precision is 0.27, recall is 0.33, and F1-score is 0.30. These results indicate that the model is not very good at correctly identifying deceptive stories.\n",
    "- **True Story**: Precision is 0.33, recall is 0.27, and F1-score is 0.30. Similar to the deceptive story class, the model struggles to correctly identify true stories.\n",
    "\n",
    "#### Key Observations:\n",
    "\n",
    "- The model shows poor performance across both classes with precision, recall, and F1-score all being around 0.30.\n",
    "- **Accuracy** is low, and the model’s ability to distinguish between \"True\" and \"Deceptive\" stories is not effective.\n",
    "- There are **misclassifications** across both categories, with the model incorrectly classifying both types of stories at a high rate.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "The SVM model, with the current feature set and preprocessing, does not perform well for the task of classifying stories as \"True\" or \"Deceptive.\" The accuracy of 30% indicates that the model struggles to make correct predictions.\n",
    "\n",
    "#### Potential Improvements:\n",
    "To improve the model's performance, consider the following strategies:\n",
    "\n",
    "1. **Feature Engineering**: Extract additional or more relevant features from the audio data. Features such as speech patterns, tone, and pitch could be more informative.\n",
    "2. **Hyperparameter Tuning**: Perform hyperparameter optimization using grid search or random search to improve the model's performance, particularly by adjusting the `C` parameter and kernel type for SVM.\n",
    "3. **Feature Scaling**: Ensure that all features are scaled appropriately. Using **StandardScaler** or **MinMaxScaler** could help improve the model’s performance, especially for SVMs.\n",
    "4. **Model Selection**: Consider trying other models, such as **Random Forest**, **Logistic Regression**, or **Neural Networks**, to see if they can handle the audio data more effectively.\n",
    "\n",
    "By exploring these adjustments, we can attempt to improve classification performance and create a more reliable model for predicting whether a narrated story is true or deceptive.\n"
   ],
   "id": "af02f40cb7cf79ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convolutional Neural Network (CNN) for audio classification",
   "id": "80f9d0a7a0c64aef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PyTorch Implementation of CNN\n",
    "In this step, we will use PyTorch to build and train a Convolutional Neural Network (CNN). The features extracted from the audio files (MFCCs) will serve as the input to the CNN. The goal is to improve upon previous performance metrics and better classify the stories.\n",
    "\n"
   ],
   "id": "66772fd7bbefa3ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:51:53.202900Z",
     "start_time": "2024-12-17T12:51:33.750745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Define file paths\n",
    "csv_file = r\"E:\\code\\truth_teller\\Deception-main\\Deception-main\\CBU0521DD_stories_attributes.csv\"\n",
    "audio_folder = r\"E:\\code\\truth_teller\\Deception-main\\Deception-main\\CBU0521DD_stories\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file, sep=\",\")\n",
    "\n",
    "# Clean column names (if needed)\n",
    "df.columns = df.columns.str.strip()  # Strip spaces in column names\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_features(audio_path, n_mfcc=13):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, duration=180)  # Limit duration to 3 minutes (180 seconds)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfccs_mean = np.mean(mfccs, axis=1)  # Compute mean of each MFCC\n",
    "        return mfccs_mean\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract MFCC features from all audio files\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    file_path = os.path.join(audio_folder, row['filename'])\n",
    "    label = row['Story_type']\n",
    "\n",
    "    # Extract features from the audio file\n",
    "    mfcc_features = extract_features(file_path)\n",
    "\n",
    "    if mfcc_features is not None:\n",
    "        features.append(mfcc_features)\n",
    "        labels.append(label)\n",
    "\n",
    "print(\"Feature extraction complete!\")\n",
    "\n",
    "# Convert features and labels to numpy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Encode labels to numeric values (Story_type)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(\"Data prepared successfully!\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "cb0388012a7f7e44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete!\n",
      "Data prepared successfully!\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define the CNN Model\n",
    "Here we define the Convolutional Neural Network architecture. Since the MFCC features are 1D arrays, we reshape them for compatibility with CNN layers."
   ],
   "id": "633fe991f4506f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:52:43.472746Z",
     "start_time": "2024-12-17T12:52:43.446993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)  # Conv Layer 1\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)  # MaxPooling\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)  # Conv Layer 2\n",
    "        self.fc1 = nn.Linear(32 * (input_size // 4), 128)  # Fully Connected Layer 1\n",
    "        self.fc2 = nn.Linear(128, 2)  # Output Layer (2 classes: True Story, Deceptive Story)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Conv1 -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Conv2 -> ReLU -> Pool\n",
    "        x = x.view(-1, 32 * (x.shape[2]))  # Flatten for FC Layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = X_tensor.shape[1]  # Input feature size\n",
    "model = AudioCNN(input_size=input_size).to(device)\n",
    "print(model)\n"
   ],
   "id": "72a1f8fed05a2b62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioCNN(\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (fc1): Linear(in_features=96, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the CNN Model\n",
    "We train the CNN using the Cross-Entropy Loss for classification and the Adam optimizer. The training loop iterates over the epochs, and loss/accuracy is tracked.\n"
   ],
   "id": "9a06096284d19132"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:52:46.376132Z",
     "start_time": "2024-12-17T12:52:46.021789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.unsqueeze(1)  # Add channel dimension for CNN (batch, channels, features)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "print(\"Training complete!\")\n"
   ],
   "id": "1029c1180ede32b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.7125\n",
      "Epoch [2/20], Loss: 0.6872\n",
      "Epoch [3/20], Loss: 0.6813\n",
      "Epoch [4/20], Loss: 0.6730\n",
      "Epoch [5/20], Loss: 0.6651\n",
      "Epoch [6/20], Loss: 0.6619\n",
      "Epoch [7/20], Loss: 0.6499\n",
      "Epoch [8/20], Loss: 0.6389\n",
      "Epoch [9/20], Loss: 0.6273\n",
      "Epoch [10/20], Loss: 0.6182\n",
      "Epoch [11/20], Loss: 0.6016\n",
      "Epoch [12/20], Loss: 0.5832\n",
      "Epoch [13/20], Loss: 0.5639\n",
      "Epoch [14/20], Loss: 0.5372\n",
      "Epoch [15/20], Loss: 0.5169\n",
      "Epoch [16/20], Loss: 0.5014\n",
      "Epoch [17/20], Loss: 0.4920\n",
      "Epoch [18/20], Loss: 0.4695\n",
      "Epoch [19/20], Loss: 0.4266\n",
      "Epoch [20/20], Loss: 0.4124\n",
      "Training complete!\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate the Model\n",
    "Here we evaluate the model's performance on the test set using metrics like accuracy, confusion matrix, and classification report."
   ],
   "id": "26423d0c21e4e686"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:52:48.890974Z",
     "start_time": "2024-12-17T12:52:48.870321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.unsqueeze(1)  # Add channel dimension\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(all_targets, all_preds)\n",
    "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "class_report = classification_report(all_targets, all_preds, target_names=[\"Deceptive Story\", \"True Story\"])\n",
    "\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n"
   ],
   "id": "3d45325d10b4a532",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.50\n",
      "Confusion Matrix:\n",
      "[[3 4]\n",
      " [6 7]]\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Deceptive Story       0.33      0.43      0.38         7\n",
      "     True Story       0.64      0.54      0.58        13\n",
      "\n",
      "       accuracy                           0.50        20\n",
      "      macro avg       0.48      0.48      0.48        20\n",
      "   weighted avg       0.53      0.50      0.51        20\n",
      "\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation Summary\n",
    "\n",
    "## Accuracy on the Test Set: 0.50\n",
    "The model achieved an accuracy of 50%, indicating that it correctly predicted half of the test samples. This is better than random guessing, but the performance is still quite low.\n",
    "\n",
    "## Confusion Matrix:\n",
    "[[3 4]\n",
    "[6 7]]\n",
    "- **True Positives (TP)**: 7 (True Story correctly predicted as True Story)\n",
    "- **False Positives (FP)**: 4 (Deceptive Story incorrectly predicted as True Story)\n",
    "- **False Negatives (FN)**: 6 (True Story incorrectly predicted as Deceptive Story)\n",
    "- **True Negatives (TN)**: 3 (Deceptive Story correctly predicted as Deceptive Story)\n",
    "\n",
    "The confusion matrix shows that while the model has a higher number of correct predictions for the \"True Story\" class, it still misclassifies a significant number of instances, particularly for the \"Deceptive Story\" class.\n",
    "- **Precision**:\n",
    "    - For \"Deceptive Story\", the precision is 0.33, meaning only 33% of the predictions for \"Deceptive Story\" are correct.\n",
    "    - For \"True Story\", the precision is higher at 0.64, indicating a better proportion of correct predictions for this class.\n",
    "\n",
    "- **Recall**:\n",
    "    - For \"Deceptive Story\", the recall is 0.43, meaning the model correctly identified 43% of the actual \"Deceptive Story\" instances.\n",
    "    - For \"True Story\", the recall is 0.54, meaning the model identified 54% of the actual \"True Story\" instances.\n",
    "\n",
    "- **F1-Score**:\n",
    "    - For \"Deceptive Story\", the F1-score is 0.38, reflecting a poor balance between precision and recall for this class.\n",
    "    - For \"True Story\", the F1-score is 0.58, indicating a better balance for the \"True Story\" class.\n",
    "\n",
    "- **Support**: The number of true instances in the test set is 7 for \"Deceptive Story\" and 13 for \"True Story\".\n",
    "\n",
    "## Macro Average:\n",
    "- The macro average precision, recall, and F1-score are all around 0.48, which is relatively low and indicates that the model is not performing well across both classes equally.\n",
    "\n",
    "## Weighted Average:\n",
    "- The weighted average precision, recall, and F1-score are higher, reflecting the model’s better performance on the \"True Story\" class due to its larger proportion in the dataset.\n",
    "\n",
    "# Insights and Next Steps\n",
    "- **Deceptive Story Misclassification**: The model is struggling with predicting \"Deceptive Story\" accurately, as shown by its low precision and recall for that class.\n",
    "- **Better Performance on True Story**: The model performs better with \"True Story\", but there is still room for improvement.\n",
    "\n",
    "# Suggestions for Improvement:\n",
    "1. **Feature Engineering**:\n",
    "   - Consider using additional audio features (e.g., Chroma, Zero Crossing Rate) to improve the model's understanding of the audio data.\n",
    "\n",
    "2. **Data Augmentation**:\n",
    "   - Apply augmentation techniques like pitch shifting, time stretching, or adding background noise to the training data to make the model more robust.\n",
    "\n",
    "3. **Model Tuning**:\n",
    "   - Experiment with different architectures or more complex models, such as **LSTM** or **GRU**, which are well-suited for sequential data like audio.\n",
    "\n",
    "4. **Hyperparameter Optimization**:\n",
    "   - Fine-tune hyperparameters like learning rate, batch size, or optimizer choice to improve model performance.\n",
    "\n",
    "5. **Balanced Dataset**:\n",
    "   - The dataset may be imbalanced, and techniques like **oversampling** or **class weighting** could help improve performance, particularly for the underrepresented class.\n",
    "\n",
    "# Conclusion\n",
    "The current model shows moderate performance with an accuracy of 50%. While it performs better on \"True Story\", it still struggles with \"Deceptive Story\". Further work on data preprocessing, model complexity, and data augmentation could help improve the overall performance.\n",
    "\n"
   ],
   "id": "3dbd682bdd96515a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f1247188ad397677"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
